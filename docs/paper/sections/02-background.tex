% Section 2: Background

\section{Background}
\label{sec:background}

This section provides background on the actor model and GPU programming,
establishing the concepts that the GPU-native actor paradigm unifies.

\subsection{The Actor Model}

The actor model~\cite{hewitt1973actors} is a mathematical model of concurrent computation.
An \emph{actor} is an autonomous computational agent with three capabilities:

\begin{enumerate}
    \item \textbf{Send messages} to actors whose addresses (``acquaintances'') it knows
    \item \textbf{Create new actors} with specified behavior
    \item \textbf{Designate behavior} for handling the next message received
\end{enumerate}

Critically, actors \emph{cannot} share state---they interact only through asynchronous
message passing. This restriction eliminates data races by construction, making reasoning
about concurrent systems tractable.

\subsubsection{Actor Semantics}

Actors process messages from a \emph{mailbox} (message queue) one at a time. Message
delivery is guaranteed but \emph{unordered}---if actor $A$ sends messages $m_1$ and $m_2$
to actor $B$, they may arrive in any order. However, within a single actor, message
processing is sequential.

The operational semantics can be expressed as a transition relation:
\begin{equation}
    \langle \mathcal{A}, \mathcal{M} \rangle \rightarrow \langle \mathcal{A}', \mathcal{M}' \rangle
\end{equation}
where $\mathcal{A}$ is the set of actors, $\mathcal{M}$ is the multiset of in-flight
messages, and the transition represents processing one message.

\subsubsection{Supervision and Fault Tolerance}

Erlang introduced the concept of \emph{supervision}~\cite{armstrong2003erlang}: actors
are organized into hierarchies where parent actors monitor children. When a child fails,
the supervisor can restart it, escalate the failure, or take other recovery actions.
This ``let it crash'' philosophy enables building fault-tolerant systems.

\subsubsection{Causal Ordering}

In distributed actor systems, establishing message ordering is essential for consistency.
\emph{Lamport timestamps}~\cite{lamport1978clocks} provide partial ordering based on
causality: if event $a$ ``happens before'' event $b$ (written $a \rightarrow b$), then
$C(a) < C(b)$ where $C$ is the clock function.

\emph{Hybrid Logical Clocks} (HLC)~\cite{kulkarni2014hlc} combine physical time with
logical counters:
\begin{equation}
    \text{HLC} = \langle \text{physical\_time}, \text{logical\_counter}, \text{node\_id} \rangle
\end{equation}
HLC provides the causal ordering guarantees of Lamport clocks while maintaining
proximity to wall-clock time. This makes HLC ideal for GPU actors where thousands
of concurrent entities require efficient causality tracking.

\subsection{GPU Architecture and Programming}

Modern GPUs contain thousands of cores organized hierarchically. While we focus on
NVIDIA CUDA terminology, the concepts generalize to other GPU platforms (AMD ROCm,
Apple Metal, WebGPU).

\subsubsection{Execution Model}

GPU computation is organized into a hierarchy:
\begin{itemize}
    \item \textbf{Thread}: Smallest execution unit, has private registers
    \item \textbf{Warp/Wavefront}: 32-64 threads executing in SIMT lockstep
    \item \textbf{Block/Workgroup}: Collection of warps sharing fast ``shared memory''
    \item \textbf{Grid}: Collection of blocks executing the same kernel
\end{itemize}

\subsubsection{Memory Hierarchy}

GPU memory is organized by access speed and scope:
\begin{itemize}
    \item \textbf{Registers}: Per-thread, fastest (1 cycle latency)
    \item \textbf{Shared/Local Memory}: Per-block, fast (5-10 cycles)
    \item \textbf{L1/L2 Cache}: Automatic caching of global memory
    \item \textbf{Global/Device Memory}: Device-wide, slow (200-400 cycles)
    \item \textbf{Mapped/Unified Memory}: Host-visible, accessible from both CPU and GPU
\end{itemize}

Mapped memory is crucial for the GPU-native actor paradigm---it enables the host to
communicate with running actors without explicit memory copies or kernel relaunches.

\subsubsection{Synchronization Primitives}

GPUs provide several synchronization mechanisms:
\begin{itemize}
    \item \textbf{Block barriers}: \texttt{\_\_syncthreads()} (CUDA), \texttt{barrier()} (OpenCL)
    \item \textbf{Atomic operations}: \texttt{atomicAdd}, \texttt{atomicCAS}, etc.
    \item \textbf{Grid-wide sync}: Cooperative groups (CUDA CC 6.0+), device-wide barriers
\end{itemize}

Grid-wide synchronization enables persistent kernels to coordinate across all blocks,
essential for iterative algorithms and actor-to-actor communication.

\subsubsection{Traditional Kernel Launch Model}

The conventional GPU programming pattern treats kernels as \emph{batch operations}:

\begin{lstlisting}[language=CUDA, caption={Traditional kernel launch pattern}]
// Host code
float *d_input, *d_output;
cudaMalloc(&d_input, size);
cudaMalloc(&d_output, size);
cudaMemcpy(d_input, h_input, size, H2D);

process_kernel<<<grid, block>>>(d_input, d_output);
cudaDeviceSynchronize();  // Block until complete

cudaMemcpy(h_output, d_output, size, D2H);
cudaFree(d_input);
cudaFree(d_output);
\end{lstlisting}

Each kernel launch involves driver API calls, command buffer submission, and
synchronization overhead---typically 10-500$\mu$s depending on GPU and driver.
This overhead is acceptable for batch workloads but prohibitive for interactive
applications requiring frequent host-device communication.

\subsection{Mapping Actors to GPUs}

Table~\ref{tab:actor-gpu-mapping} shows how actor model concepts map to GPU primitives,
forming the conceptual foundation of the GPU-native actor paradigm:

\begin{table}[h]
\centering
\caption{Mapping between Actor Model and GPU concepts}
\label{tab:actor-gpu-mapping}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Actor Concept} & \textbf{GPU Equivalent} \\
\midrule
Actor & Persistent thread block \\
Private state & Shared memory + registers \\
Mailbox & Lock-free ring buffer in global/mapped memory \\
Message send & Atomic enqueue operation \\
Message receive & Atomic dequeue operation \\
Actor creation & Pre-allocated actor pool (dynamic limited) \\
Supervision & Host thread monitoring ControlBlock \\
Causal ordering & GPU-resident HLC timestamps \\
\bottomrule
\end{tabular}
\end{table}

This mapping enables treating GPU thread blocks as first-class actors with full
actor semantics---message passing, private state, and causal ordering---while
leveraging GPU parallelism for massive concurrency.

\subsection{The Paradigm Shift}

The GPU-native actor paradigm represents a fundamental shift in how we think about
GPU programming:

\begin{table}[h]
\centering
\caption{Traditional GPU vs GPU-Native Actor paradigm}
\label{tab:paradigm-shift}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Aspect} & \textbf{Traditional GPU} & \textbf{GPU-Native Actors} \\
\midrule
Kernel lifetime & Milliseconds & Indefinite (persistent) \\
Communication & Memory copies & Message queues \\
State location & Host memory & GPU-resident \\
Interaction & Launch-per-operation & Continuous messaging \\
Latency model & Batch amortization & Sub-microsecond commands \\
Programming model & Data parallelism & Actor concurrency \\
\bottomrule
\end{tabular}
\end{table}

This shift enables new application classes: interactive simulations, real-time
analytics, living databases, and distributed GPU systems---all benefiting from
actor semantics while exploiting GPU parallelism.
