% Section 5: Implementation

\section{Implementation}
\label{sec:implementation}

RingKernel is implemented in Rust with approximately 25,000 lines of code across
multiple crates. This section details key implementation aspects.

\subsection{Crate Architecture}

RingKernel is organized as a Cargo workspace:

\begin{itemize}
    \item \texttt{ringkernel-core}: Core traits, types, and enterprise features (457 tests)
    \item \texttt{ringkernel-derive}: Procedural macros for actor definitions
    \item \texttt{ringkernel-cpu}: CPU backend for testing and fallback
    \item \texttt{ringkernel-cuda}: NVIDIA CUDA backend with cudarc bindings
    \item \texttt{ringkernel-cuda-codegen}: Rust-to-CUDA transpiler (190+ tests)
    \item \texttt{ringkernel-wgpu}: WebGPU cross-platform backend
    \item \texttt{ringkernel-wgpu-codegen}: Rust-to-WGSL transpiler
    \item \texttt{ringkernel}: Facade crate re-exporting all functionality
\end{itemize}

\subsection{Rust-to-CUDA Transpilation}

The transpiler converts Rust DSL to CUDA C, enabling developers to write GPU
kernels in familiar syntax while generating optimized device code.

\subsubsection{Input: Rust Actor Definition}

\begin{lstlisting}[language=Rust, caption={Rust actor definition}]
#[ring_kernel(id = "processor", block_size = 128)]
fn handle(ctx: &RingContext, msg: &Request) -> Response {
    let tid = ctx.global_thread_id();

    // Shared memory reduction
    let partial = shared_reduce(msg.values, ReductionOp::Sum);

    ctx.sync_threads();

    if tid == 0 {
        Response { sum: partial, count: msg.values.len() }
    }
}
\end{lstlisting}

\subsubsection{Output: CUDA Kernel}

The transpiler generates a persistent kernel with message handling:

\begin{lstlisting}[language=CUDA, caption={Generated CUDA kernel (simplified)}]
extern "C" __global__ void processor_kernel(
    ControlBlock* cb,
    RingBuffer* h2k_queue,
    RingBuffer* k2h_queue,
    K2KRouter* k2k_router
) {
    __shared__ uint8_t shared_mem[4096];

    // Initialize HLC
    HlcClock hlc = {0, 0, cb->kernel_id};

    // Persistent message loop
    while (true) {
        // Check termination
        if (cb->state == STATE_TERMINATED) break;

        // Try receive from H2K
        MessageHeader header;
        if (try_dequeue(h2k_queue, &header)) {
            hlc_receive(&hlc, &header);

            switch (header.message_type) {
            case MSG_REQUEST:
                Request* req = (Request*)(h2k_queue->data
                    + header.payload_offset);
                Response resp = handle_request(req);

                // Send response
                MessageHeader resp_hdr;
                resp_hdr.correlation_id = header.correlation_id;
                hlc_send(&hlc, &resp_hdr);
                enqueue(k2h_queue, &resp_hdr, &resp);
                break;

            case MSG_TERMINATE:
                cb->state = STATE_TERMINATED;
                break;
            }
        }

        // Update heartbeat
        if (threadIdx.x == 0) {
            atomicExch(&cb->heartbeat, cb->system_time);
        }
    }
}
\end{lstlisting}

\subsubsection{Transpilation Pipeline}

The transpiler operates in phases:

\begin{enumerate}
    \item \textbf{Parse}: Convert Rust source to AST using \texttt{syn}
    \item \textbf{Analyze}: Extract types, identify GPU intrinsics, validate semantics
    \item \textbf{Transform}: Convert Rust constructs to CUDA equivalents
    \item \textbf{Generate}: Emit CUDA C source code
    \item \textbf{Compile}: Invoke \texttt{nvcc} to produce PTX
\end{enumerate}

\subsubsection{Intrinsic Mapping}

The transpiler maps 120+ GPU intrinsics across categories:

\begin{table}[h]
\centering
\caption{GPU intrinsic mapping examples}
\label{tab:intrinsics}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Rust DSL} & \textbf{CUDA Output} \\
\midrule
\texttt{ctx.thread\_id()} & \texttt{threadIdx.x} \\
\texttt{ctx.block\_id()} & \texttt{blockIdx.x} \\
\texttt{ctx.sync\_threads()} & \texttt{\_\_syncthreads()} \\
\texttt{ctx.atomic\_add(\&x, v)} & \texttt{atomicAdd(\&x, v)} \\
\texttt{ctx.warp\_shuffle(v, lane)} & \texttt{\_\_shfl\_sync(0xffffffff, v, lane)} \\
\texttt{pos.north(buf)} & \texttt{buf[(y-1)*width + x]} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Memory Management}

\subsubsection{Mapped Memory for Zero-Copy}

RingKernel uses CUDA mapped memory for H2K/K2H queues:

\begin{lstlisting}[language=Rust, caption={Mapped memory allocation}]
// Allocate mapped memory visible to both CPU and GPU
let h2k_buffer = device.alloc_mapped::<u8>(QUEUE_SIZE)?;
let k2h_buffer = device.alloc_mapped::<u8>(QUEUE_SIZE)?;

// CPU can write directly, GPU sees changes
h2k_buffer.host_ptr().write(message);
// Memory fence ensures visibility
std::sync::atomic::fence(Ordering::SeqCst);
\end{lstlisting}

\subsubsection{Stratified Memory Pooling}

For analytics workloads, RingKernel provides size-stratified buffer pools:

\begin{lstlisting}[language=Rust, caption={Stratified memory pool}]
pub enum SizeBucket {
    Tiny,   // 256 bytes
    Small,  // 1 KB
    Medium, // 4 KB
    Large,  // 16 KB
    Huge,   // 64 KB
}

impl StratifiedMemoryPool {
    pub fn allocate(&self, size: usize) -> StratifiedBuffer {
        let bucket = SizeBucket::for_size(size);
        self.buckets[bucket].try_pop()
            .unwrap_or_else(|| self.alloc_new(bucket))
    }
}
\end{lstlisting}

This reduces allocation overhead for repeated operations.

\subsection{Cooperative Groups Integration}

For grid-wide synchronization, RingKernel uses CUDA cooperative groups:

\begin{lstlisting}[language=CUDA, caption={Cooperative groups for grid sync}]
#include <cooperative_groups.h>
namespace cg = cooperative_groups;

__global__ void persistent_stencil(ControlBlock* cb, ...) {
    cg::grid_group grid = cg::this_grid();

    while (cb->state == STATE_ACTIVE) {
        // Phase 1: Compute
        compute_stencil(local_tile);

        // Grid-wide barrier
        grid.sync();

        // Phase 2: Exchange halos
        exchange_halos(k2k_router);

        grid.sync();

        // Update step counter
        if (threadIdx.x == 0 && blockIdx.x == 0) {
            atomicAdd(&cb->step_counter, 1);
        }
    }
}
\end{lstlisting}

Cooperative launch requires special invocation:

\begin{lstlisting}[language=Rust, caption={Cooperative kernel launch}]
// Check device supports cooperative launch
let props = device.properties();
assert!(props.cooperative_launch != 0);

// Launch with cooperative API
unsafe {
    cudarc::driver::result::launch_cooperative_kernel(
        func,
        (grid_x, grid_y, grid_z),
        (block_x, block_y, block_z),
        shared_mem_bytes,
        stream,
        kernel_params.as_mut_ptr(),
    )?;
}
\end{lstlisting}

\subsection{K2K Message Routing}

Kernel-to-kernel messaging uses a routing table in device memory:

\begin{lstlisting}[language=CUDA, caption={K2K routing}]
struct K2KRouteEntry {
    uint32_t dest_kernel_id;
    uint32_t buffer_offset;
    uint32_t buffer_size;
    uint32_t flags;
};

__device__ bool k2k_send(K2KRouter* router, uint32_t dest,
                          MessageHeader* msg, void* payload) {
    // Find route
    K2KRouteEntry* route = find_route(router, dest);
    if (!route) return false;

    // Enqueue to destination's buffer
    RingBuffer* dest_buf = (RingBuffer*)(router->base
        + route->buffer_offset);
    return enqueue(dest_buf, msg, payload);
}
\end{lstlisting}

For 3D stencil computations, K2K enables halo exchange between neighboring tiles
without host involvement, critical for persistent FDTD simulations.

\subsection{Enterprise Features}

RingKernel includes production-ready infrastructure:

\subsubsection{Health Monitoring}

\begin{lstlisting}[language=Rust, caption={Kernel watchdog}]
pub struct KernelWatchdog {
    timeout: Duration,
    last_heartbeat: Instant,
}

impl KernelWatchdog {
    pub fn check(&mut self, cb: &ControlBlock) -> HealthStatus {
        let heartbeat = cb.heartbeat.load(Ordering::Acquire);
        if self.last_heartbeat.elapsed() > self.timeout
           && heartbeat == self.last_seen_heartbeat {
            HealthStatus::Stalled
        } else {
            self.last_seen_heartbeat = heartbeat;
            HealthStatus::Healthy
        }
    }
}
\end{lstlisting}

\subsubsection{Circuit Breaker}

\begin{lstlisting}[language=Rust, caption={Circuit breaker pattern}]
pub struct CircuitBreaker {
    state: AtomicU8,  // Closed, Open, HalfOpen
    failure_count: AtomicU32,
    threshold: u32,
    reset_timeout: Duration,
}

impl CircuitBreaker {
    pub fn execute<F, R>(&self, f: F) -> Result<R>
    where F: FnOnce() -> Result<R> {
        match self.state.load(Ordering::Acquire) {
            OPEN => Err(Error::CircuitOpen),
            HALF_OPEN | CLOSED => {
                match f() {
                    Ok(r) => { self.record_success(); Ok(r) }
                    Err(e) => { self.record_failure(); Err(e) }
                }
            }
        }
    }
}
\end{lstlisting}

\subsubsection{Observability}

RingKernel integrates with Prometheus and OpenTelemetry:

\begin{lstlisting}[language=Rust, caption={Metrics export}]
// Prometheus metrics
let metrics = PrometheusExporter::new();
metrics.register_counter("ringkernel_messages_processed");
metrics.register_histogram("ringkernel_message_latency_us");

// OpenTelemetry tracing
let tracer = OtlpExporter::new("http://jaeger:4317");
let span = tracer.start_span("process_message");
span.set_attribute("kernel_id", kernel_id);
\end{lstlisting}

\subsection{WebGPU Backend}

For cross-platform support, RingKernel includes a WebGPU backend via wgpu:

\begin{lstlisting}[language=Rust, caption={WebGPU runtime}]
pub struct WgpuRuntime {
    device: wgpu::Device,
    queue: wgpu::Queue,
    compute_pipeline: wgpu::ComputePipeline,
}

impl RingKernelRuntime for WgpuRuntime {
    async fn launch(&self, kernel: &str, opts: LaunchOptions)
        -> Result<KernelHandle> {
        // WebGPU doesn't support true persistent kernels
        // Emulate with host-driven dispatch loop
        let handle = EmulatedPersistentHandle::new(
            self.device.clone(),
            self.compute_pipeline.clone(),
        );
        Ok(KernelHandle::Emulated(handle))
    }
}
\end{lstlisting}

WebGPU limitations (no persistent kernels, no 64-bit atomics) are documented
and worked around where possible.

\subsection{Testing Infrastructure}

RingKernel has 900+ tests across the workspace:

\begin{itemize}
    \item \textbf{Unit tests}: Core logic, transpiler passes
    \item \textbf{Integration tests}: End-to-end kernel execution
    \item \textbf{Property tests}: Queue invariants via \texttt{proptest}
    \item \textbf{GPU tests}: Require hardware, use \texttt{\#[ignore]}
\end{itemize}

\begin{lstlisting}[language=Rust, caption={Property-based queue testing}]
proptest! {
    #[test]
    fn queue_fifo_order(messages: Vec<TestMessage>) {
        let queue = MessageQueue::new(1024);
        for msg in &messages {
            queue.enqueue(msg.clone()).unwrap();
        }
        for expected in &messages {
            let actual = queue.dequeue().unwrap();
            prop_assert_eq!(actual, *expected);
        }
    }
}
\end{lstlisting}

\subsection{Cross-Language Ecosystem}

The GPU-native actor paradigm is implemented across multiple languages and frameworks,
sharing common architectural patterns while adapting to language-specific idioms.

\subsubsection{Shared Architecture Patterns}

All implementations share these core patterns:

\begin{enumerate}
    \item \textbf{ControlBlock}: A 128-256 byte GPU-resident structure managing actor
    lifecycle (state, heartbeat, step counter, configuration).

    \item \textbf{Ring Buffer Queues}: Lock-free SPSC queues with atomic head/tail
    pointers and power-of-2 capacity for efficient modulo operations.

    \item \textbf{Message Envelope}: 64-256 byte headers with magic number, type ID,
    correlation ID, HLC timestamp, and checksum for validation.

    \item \textbf{HLC Integration}: Hybrid Logical Clocks with physical time, logical
    counter, and node ID---updated on every send/receive operation.

    \item \textbf{Persistent Dispatch Loop}: The kernel runs indefinitely, polling
    for messages and updating heartbeat to signal liveness.
\end{enumerate}

\subsubsection{DotCompute (.NET Implementation)}

DotCompute implements the paradigm for .NET 9+ with C\# idioms:

\begin{itemize}
    \item \textbf{Source Generators}: Roslyn-based compile-time kernel generation
    from C\# method signatures with \texttt{[RingKernel]} attributes

    \item \textbf{LINQ-to-GPU}: Expression tree analysis converts LINQ queries
    to fused GPU kernels, reducing memory bandwidth 50-80\%

    \item \textbf{MessageQueueBridge}: Named queues with background DMA transfer
    thread, achieving 1.24$\mu$s serialization latency

    \item \textbf{Native AOT}: Full support for ahead-of-time compilation with
    trimming, enabling sub-10ms cold start

    \item \textbf{Multi-Backend}: Unified API across CUDA, OpenCL, Metal, and
    CPU fallback via backend abstraction layer
\end{itemize}

\subsubsection{Orleans.GpuBridge (Distributed Actors)}

Orleans.GpuBridge extends Microsoft Orleans with GPU-native grains:

\begin{itemize}
    \item \textbf{RingKernelGrainBase}: Base class for Orleans grains backed by
    persistent GPU kernels, with automatic lifecycle integration

    \item \textbf{Hypergraph Actors}: Multi-way relationships stored in CSR
    (Compressed Sparse Row) format with GPU-accelerated pattern matching

    \item \textbf{P2P GPU Messaging}: K2KDispatcher routes messages via NVLink,
    PCIe P2P, or CPU fallback based on topology discovery

    \item \textbf{GPU Telemetry}: Per-grain memory tracking with OpenTelemetry
    export for production monitoring

    \item \textbf{Polly v8 Resilience}: Circuit breakers, retry policies, and
    rate limiting adapted for GPU failure modes
\end{itemize}

\subsubsection{RustGraph (Living Graph Database)}

RustGraph applies GPU-native actors to graph analytics:

\begin{itemize}
    \item \textbf{GpuNodeState}: 256-byte per-node actor state (\texttt{\#[repr(C, align(256))]})
    with 40+ inline analytics fields including PageRank, eigenvector centrality, component ID,
    BFS distance, triangle count, fraud triangle score, control coverage, and HLC timestamps

    \item \textbf{Per-Node Inboxes}: Each graph node has a K2K ring buffer
    (512 slots default) for receiving neighbor messages via lock-free atomics

    \item \textbf{Living Analytics}: 64+ algorithms across 15 domains (centrality, community,
    components, traversal, similarity, GNN, accounting, compliance, process mining, behavioral,
    temporal, audit) maintained via continuous message propagation---queries read current state in O(1)

    \item \textbf{Unified Hypergraph}: Three interconnected domains in a single GPU-resident structure:
    \begin{itemize}
        \item \textit{Accounting}: Vendor, Customer, Account, JournalEntry, JournalLine (types 1-204)
        \item \textit{ICS}: Control, Risk, Assertion, ControlObjective (types 300-303)
        \item \textit{OCPM}: Process, Activity, Event, ObjectType (types 400-403)
    \end{itemize}
    Connected via 37 edge types including CoversAccount, MitigatesRisk, HasActivity, InvolvesObject,
    with 26 fraud labels encoded in bitmap for GPU-side detection

    \item \textbf{Process Mining}: Object-Centric Process Mining (OCPM) with multi-object patterns
    tracking P2P, O2C, R2R, and custom processes through activity sequences
\end{itemize}

\subsubsection{Comprehensive Analytics Suite}

RustGraph's unified hypergraph demo demonstrates 20 production-ready analytics across 6 categories:

\paragraph{GPU Living Analytics (3)}
PageRank, Connected Components, and BFS execute continuously as living graph actors,
maintaining always-current state queryable in O(1) time (3--17 ns per query).

\paragraph{Behavioral Analytics (5)}
\begin{itemize}
    \item \textbf{Behavioral Profiling}: Entity-level activity pattern extraction
    \item \textbf{Isolation Forest}: GPU-accelerated anomaly detection (100 trees, 256 samples/tree)
    \item \textbf{Fraud Signatures}: Pattern matching for known fraud schemes
    \item \textbf{Causal Graph}: Dependency analysis for root cause identification
    \item \textbf{Forensic Query}: Path-based investigation from flagged nodes
\end{itemize}

\paragraph{Temporal Analytics (2)}
\begin{itemize}
    \item \textbf{Change Point Detection}: Identify significant state transitions via per-node history rings
    \item \textbf{Event Correlation}: Cross-domain temporal pattern matching
\end{itemize}

\paragraph{Audit Analytics (6)---ISA 240/315/570, SOX 404}
\begin{itemize}
    \item \textbf{Fraud Triangle}: Opportunity + Pressure + Rationalization scoring
    \item \textbf{Three-Way Match}: PO-GR-Invoice validation with tolerance matching
    \item \textbf{SoD Analysis}: Segregation of duties conflict detection
    \item \textbf{Going Concern}: Financial health indicators (ISA 570)
    \item \textbf{Control Coverage}: Maps controls to accounts/processes, identifies gaps
    \item \textbf{Deficiency Classification}: MW/SD/CD classification per SOX 404
\end{itemize}

\paragraph{Compliance Analytics (3)---AML/KYC}
\begin{itemize}
    \item \textbf{AML Detection}: Structuring detection, layering patterns, rapid movement
    \item \textbf{KYC Scoring}: 10-factor risk assessment (PEP, sanctions, geographic risk)
    \item \textbf{Circular Flow Detection}: SCC-based money laundering ring identification
\end{itemize}

\paragraph{Accounting Analytics (3)}
\begin{itemize}
    \item \textbf{GL Reconciliation}: Multi-method matching with confidence scoring
    \item \textbf{GAAP Violation Detection}: Balance checking, single-sided entries, round number flagging
    \item \textbf{Suspense Account Detection}: Turnover ratio analysis, pass-through detection
\end{itemize}

\subsubsection{Audit/Compliance Implementation Details}

The audit analytics leverage the GPU-resident unified hypergraph for cross-domain queries:

\begin{lstlisting}[language=Rust, caption={Fraud Triangle scoring via GPU actor state}]
// GpuNodeState includes inline audit fields (256 bytes total)
#[repr(C, align(256))]
struct GpuNodeState {
    // ... identity, topology, analytics fields ...

    // Audit fields (computed via living analytics)
    fraud_triangle_score: f32,   // 0.0-1.0 composite risk
    control_coverage: f32,       // % controls active
    three_way_match: u8,         // 0=pending, 1=matched, 2=exception
    sod_violations: u8,          // Count of active violations

    // Compliance fields
    aml_risk_score: f32,         // AML risk level
    kyc_tier: u8,                // 1=Low, 2=Medium, 3=High, 4=Prohibited
}
\end{lstlisting}

The unified hypergraph enables queries such as: ``Find all vendors with high fraud
triangle scores whose payments flow through accounts lacking control coverage,''
executed via single CSR traversal with GPU-resident state access.

\subsubsection{P0-P4 GPU Optimizations}

RustGraph implements five GPU optimization levels based on the research in
``Optimizing GPU Living Actor Systems for Scalability and Performance'':

\paragraph{P0: Fused Multi-Algorithm Kernels}
A single memory pass executes PageRank, Connected Components, and BFS simultaneously
via an \texttt{active\_algos} bitmask (\texttt{ALGO\_PAGERANK=1, ALGO\_CC=2, ALGO\_EIGENVECTOR=4, ALGO\_BFS=8}).
This eliminates redundant memory transfers and achieves \textbf{3.51$\times$ speedup}
(target: 1.5--2.5$\times$) by amortizing CSR traversal cost across algorithms.

\paragraph{P1: Hybrid Dispatch with Node Classification}
Nodes are classified by degree into three tiers:
\begin{itemize}
    \item Regular ($<$512 degree): Standard node-centric processing
    \item Hub ($\geq$512 degree): Edge-centric kernels with warp-cooperative primitives
    \item SuperHub ($\geq$4096 degree): Specialized handling with work distribution
\end{itemize}
This addresses the load imbalance inherent in scale-free graphs where hub nodes
can dominate processing time.

\paragraph{P2: Work Stealing Between Warps}
A 512-byte GPU-resident \texttt{GlobalWorkStealingState} structure enables:
\begin{itemize}
    \item Block overflow bitmap for identifying overloaded nodes
    \item Idle node bitmap for locating available workers
    \item Adaptive threshold adjustment based on queue lengths
\end{itemize}
Result: \textbf{68\% steal success rate} (target: 50--70\%), improving GPU occupancy
for workloads with heterogeneous node degrees.

\paragraph{P3: Async Convergence Checking}
Warp-local convergence detection with speculative iteration continuation:
\begin{itemize}
    \item Each warp maintains local convergence state
    \item Speculative execution continues while awaiting global sync
    \item Early termination when warp determines local convergence
\end{itemize}
Result: \textbf{80\% synchronization reduction} (target: 60\%), critical for
algorithms like PageRank where most nodes converge before the global check.

\paragraph{P4: Multi-GPU Partitioning}
METIS-based graph partitioning for multi-GPU execution:
\begin{itemize}
    \item Minimize edge cuts between partitions
    \item \texttt{tree\_reduce()} for cross-GPU aggregation
    \item P2P communication via NVLink when available
\end{itemize}
Result: \textbf{0.0\% partition imbalance} (target: $<$5\%), enabling linear
scaling to multiple GPUs.

\paragraph{Kernel Mode Selection}
The system automatically selects the optimal kernel mode based on graph characteristics:

\begin{lstlisting}[language=Rust, caption={Automatic kernel mode selection}]
pub enum KernelMode {
    NodeCentric,  // 1 thread per node (default)
    SoA,          // Coalesced memory via Structure-of-Arrays
    EdgeCentric,  // 1 thread per edge (for hubs)
    Tiled,        // L2 cache blocking with __ldg()
    Auto,         // Automatic selection
}

fn select_optimal_kernel(stats: &GraphStats) -> KernelMode {
    if stats.max_degree > 512 { EdgeCentric }
    else if stats.working_set > 2 * L2_CACHE { Tiled }
    else if stats.working_set > L2_CACHE { SoA }
    else { NodeCentric }
}
\end{lstlisting}

\subsubsection{Code Generation Comparison}

Table~\ref{tab:codegen-comparison} compares the code generation approaches:

\begin{table}[h]
\centering
\caption{Code generation approaches across implementations}
\label{tab:codegen-comparison}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{System} & \textbf{Input} & \textbf{Output} \\
\midrule
RingKernel & Rust DSL + proc macros & CUDA C, WGSL \\
DotCompute & C\# + source generators & CUDA, OpenCL, Metal \\
Orleans.GpuBridge & C\# + DotCompute & Via DotCompute \\
RustGraph & Rust DSL & CUDA PTX \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Combined Test Coverage}

The ecosystem maintains comprehensive test coverage:

\begin{itemize}
    \item \textbf{RingKernel}: 900+ tests (Rust)
    \item \textbf{DotCompute}: 215/234 tests passing (C\#)
    \item \textbf{Orleans.GpuBridge}: 1,231 tests (C\#)
    \item \textbf{RustGraph}: 1,400+ tests (Rust)
    \item \textbf{Total}: 3,700+ tests across the ecosystem
\end{itemize}

This cross-language implementation demonstrates that the GPU-native actor paradigm
is not language-specific but a universal pattern applicable wherever persistent
GPU kernels and lock-free messaging are available.

\subsection{Application Layer: RustAssureTwin}

While the previous sections describe the engine and infrastructure layers, an important
validation of any systems paradigm is whether it can serve domain experts who are not
systems programmers. \textbf{RustAssureTwin} is a native desktop audit intelligence
platform that consumes RustGraph's GPU-native actor state to provide professional
auditors with a \emph{digital twin} of the organization under audit.

\subsubsection{Architecture}

RustAssureTwin is built on a four-layer architecture:

\begin{enumerate}
    \item \textbf{Presentation Layer} (Svelte~5 + Tailwind~CSS): Seven application modules
    (Dashboard, Explorer, Process, Controls, Audit, Reports, Admin) rendered in a
    Tauri~2 desktop shell with wgpu-accelerated graph visualization supporting 100K+ nodes
    at 60~FPS.

    \item \textbf{Application Core} (TypeScript + Svelte stores): 21 reactive stores
    managing graph state, temporal context, AI assistant, workflow progress, and
    cross-module navigation. Stores subscribe to RustGraph state changes via
    WebSocket and expose O(1) query results to UI components.

    \item \textbf{Data Access Layer} (Rust backend via Tauri IPC): HTTP and WebSocket
    clients to RustGraph, local SQLite cache for offline operation, and bulk
    import/export for disconnected audit scenarios.

    \item \textbf{Living Graph Engine} (RustGraph): GPU-native actors maintaining
    64+ analytics algorithms continuously, providing the always-current state that
    the application layer reads in O(1) time.
\end{enumerate}

Communication between layers uses Tauri's IPC mechanism: the Svelte frontend calls
\texttt{invoke()} to the Rust backend, which in turn queries RustGraph via HTTP/WebSocket.
This architecture ensures that GPU actor state reaches the UI within a single frame budget
(16.67ms at 60~FPS), validated by the mixed-workload evaluation in Section~\ref{sec:evaluation}.

\subsubsection{AI Agent Architecture}

RustAssureTwin integrates a three-tier AI agent system that is a first-class consumer of
GPU-native actor state:

\begin{itemize}
    \item \textbf{Tier~1 --- Observational}: Pattern detection and anomaly flagging.
    The AI reads O(1) living analytics (fraud triangle scores, control coverage, SoD
    violations) and surfaces findings to auditors. This tier requires no write access
    to the graph and operates within ISA~200 professional skepticism guidelines.

    \item \textbf{Tier~2 --- Analytical}: Risk assessment and sampling recommendations.
    The AI aggregates GPU-computed analytics across the unified hypergraph to recommend
    sample sizes (per ISA~530) and identify high-risk areas. Confidence thresholds
    gate AI suggestions: $\geq$0.95 for factual queries, $\geq$0.80 for analytical
    suggestions, $\geq$0.70 for exploratory recommendations.

    \item \textbf{Tier~3 --- Collaborative}: Workpaper drafting and finding
    documentation. The AI proposes structured content for audit workpapers, but all
    output requires explicit human approval before inclusion---enforcing the
    human-in-the-loop governance required by ISA~220 and PCAOB AS~1201.
\end{itemize}

The AI agents benefit directly from the GPU actor paradigm: because analytics are maintained
continuously (not computed on-demand), the AI can access fraud scores, control coverage,
and process conformance metrics in 3--17~ns per query. This enables conversational-speed
AI interactions where the assistant can traverse the full unified hypergraph context
within a single user-perceived response latency.

\subsubsection{Temporal Visualization Pipeline}

The bi-temporal model described in Section~\ref{sec:design} is surfaced through an
interactive temporal visualization pipeline:

\begin{enumerate}
    \item \textbf{Temporal bar}: A global timeline control in the application footer
    allows auditors to scrub to any point in valid time, transaction time, or business
    period. Changing the temporal context updates all modules simultaneously.

    \item \textbf{Playback controls}: Step-by-step temporal navigation with configurable
    granularity (event, day, week, month) and playback speed (0.25$\times$--8$\times$).
    The wgpu renderer animates graph state transitions by interpolating between
    per-node history ring entries.

    \item \textbf{Bi-temporal timeline}: A dual-axis visualization showing valid time
    (horizontal) vs transaction time (vertical), enabling auditors to identify
    retroactive journal entries---transactions where $t_v \ll t_t$, a key fraud
    indicator under ISA~240.
\end{enumerate}

This pipeline demonstrates a concrete consumer of the per-node history rings: each
ring entry's HLC timestamp drives the animation, while the application layer maps
HLC values to human-readable dates and fiscal periods.

\subsubsection{Audit Workflow Integration}

RustAssureTwin implements complete audit workflows that consume GPU-native actor state:

\begin{itemize}
    \item \textbf{Audit Scoping}: A scope wizard identifies Significant Classes of
    Transactions (SCOTs) by querying GPU-resident entity types and their risk scores.
    A cost model editor estimates audit effort using role rates and SCOT complexity.

    \item \textbf{Control Testing}: Step-by-step test execution panels guide auditors
    through control tests, with test results written back to the graph as audit
    evidence nodes linked to Control entities (types 300--303).

    \item \textbf{Workpaper Management}: Section-based editors with embedded graph
    snapshots---auditors can capture the current state of any subgraph and embed it
    as evidence in ISA-compliant workpapers.

    \item \textbf{Cross-Module Navigation}: A universal ``View in Explorer'' pattern
    enables drill-down from any entity reference (control ID, finding, risk score)
    to the Explorer module's graph visualization, preserving temporal and filter context
    via URL state synchronization.
\end{itemize}

\subsubsection{Test Infrastructure}

RustAssureTwin maintains 456 unit tests, 48 integration tests, and a Playwright E2E
test suite covering navigation, performance, and audit-specific workflows. The E2E tests
validate that GPU actor state reaches the UI correctly: performance tests verify that
all 7 application modules load within 3 seconds, canvas interactions respond within
500ms, and the graph visualization survives rapid zoom/pan stress tests without memory
issues.
