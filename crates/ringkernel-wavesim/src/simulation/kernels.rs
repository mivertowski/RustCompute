//! CUDA kernel definitions using the Rust DSL.
//!
//! This module contains all CUDA kernels for the wave simulation, defined in
//! a Rust DSL that gets transpiled to CUDA C at compile time.
//!
//! The generated CUDA code is designed to match the handwritten versions in
//! `shaders/fdtd_tile.cu` and `shaders/fdtd_packed.cu` exactly.

#[cfg(feature = "cuda-codegen")]
use ringkernel_cuda_codegen::{transpile_stencil_kernel, Grid, StencilConfig};

// ============================================================================
// Tile-Based Kernels (fdtd_tile.cu equivalent)
// ============================================================================

/// Generate the complete CUDA source for tile-based kernels.
///
/// This generates CUDA code equivalent to `shaders/fdtd_tile.cu`:
/// - `fdtd_tile_step`: Main FDTD wave equation kernel
/// - `extract_halo`: Extract halo from interior edge
/// - `inject_halo`: Inject halo to boundary region
/// - `read_interior`: Read interior cells to output buffer
/// - `apply_boundary_reflection`: Apply boundary conditions
#[cfg(feature = "cuda-codegen")]
pub fn generate_tile_kernels() -> String {
    let mut output = String::new();

    output.push_str(TILE_KERNELS_HEADER);
    output.push_str("\nextern \"C\" {\n\n");

    // Generate fdtd_tile_step kernel
    output.push_str(&generate_fdtd_tile_step());
    output.push('\n');

    // Generate extract_halo kernel
    output.push_str(&generate_extract_halo());
    output.push('\n');

    // Generate inject_halo kernel
    output.push_str(&generate_inject_halo());
    output.push('\n');

    // Generate read_interior kernel
    output.push_str(&generate_read_interior());
    output.push('\n');

    // Generate apply_boundary_reflection kernel
    output.push_str(&generate_apply_boundary_reflection());

    output.push_str("\n}  // extern \"C\"\n");

    output
}

/// Header comment for generated tile kernels.
pub const TILE_KERNELS_HEADER: &str = r#"// CUDA Kernels for Tile-Based FDTD Wave Simulation
// Generated by ringkernel-cuda-codegen from Rust DSL
//
// Buffer Layout (18x18 = 324 floats):
//   +---+----------------+---+
//   | NW|   North Halo   |NE |  <- Row 0
//   +---+----------------+---+
//   |   |   16x16 Tile   |   |  <- Rows 1-16
//   | W |    Interior    | E |
//   +---+----------------+---+
//   | SW|   South Halo   |SE |  <- Row 17
//   +---+----------------+---+
//
// Index: idx = y * 18 + x
// Interior cell (lx, ly): idx = (ly + 1) * 18 + (lx + 1)
"#;

/// Generate the main FDTD tile step kernel.
#[cfg(feature = "cuda-codegen")]
fn generate_fdtd_tile_step() -> String {
    use syn::parse_quote;

    let kernel_fn: syn::ItemFn = parse_quote! {
        fn fdtd_tile_step(
            pressure: &[f32],
            pressure_prev: &mut [f32],
            c2: f32,
            damping: f32,
            pos: GridPos,
        ) {
            let p = pressure[pos.idx()];
            let p_prev = pressure_prev[pos.idx()];

            let p_n = pos.north(pressure);
            let p_s = pos.south(pressure);
            let p_w = pos.west(pressure);
            let p_e = pos.east(pressure);

            let laplacian = p_n + p_s + p_e + p_w - 4.0 * p;
            let p_new = 2.0 * p - p_prev + c2 * laplacian;

            pressure_prev[pos.idx()] = p_new * damping;
        }
    };

    let config = StencilConfig::new("fdtd_tile_step")
        .with_grid(Grid::Grid2D)
        .with_tile_size(16, 16)
        .with_halo(1);

    match transpile_stencil_kernel(&kernel_fn, &config) {
        Ok(cuda) => cuda,
        Err(e) => format!("// Transpilation error: {}\n", e),
    }
}

/// Generate the extract_halo kernel.
#[cfg(feature = "cuda-codegen")]
fn generate_extract_halo() -> String {
    // This kernel uses switch/case which needs manual generation
    // since our DSL doesn't support match expressions yet
    r#"// Extract halo data from interior edge to linear buffer
// edge: 0=North, 1=South, 2=West, 3=East
__global__ void extract_halo(
    const float* __restrict__ pressure,
    float* __restrict__ halo_out,
    int edge
) {
    int i = threadIdx.x;
    if (i >= 16) return;

    int idx;
    switch (edge) {
        case 0:  // North - extract row 1
            idx = 1 * 18 + (i + 1);
            break;
        case 1:  // South - extract row 16
            idx = 16 * 18 + (i + 1);
            break;
        case 2:  // West - extract col 1
            idx = (i + 1) * 18 + 1;
            break;
        case 3:  // East - extract col 16
        default:
            idx = (i + 1) * 18 + 16;
            break;
    }

    halo_out[i] = pressure[idx];
}
"#.to_string()
}

/// Generate the inject_halo kernel.
#[cfg(feature = "cuda-codegen")]
fn generate_inject_halo() -> String {
    r#"// Inject halo data from linear buffer to halo region
// edge: 0=North, 1=South, 2=West, 3=East
__global__ void inject_halo(
    float* __restrict__ pressure,
    const float* __restrict__ halo_in,
    int edge
) {
    int i = threadIdx.x;
    if (i >= 16) return;

    int idx;
    switch (edge) {
        case 0:  // North - inject to row 0
            idx = 0 * 18 + (i + 1);
            break;
        case 1:  // South - inject to row 17
            idx = 17 * 18 + (i + 1);
            break;
        case 2:  // West - inject to col 0
            idx = (i + 1) * 18 + 0;
            break;
        case 3:  // East - inject to col 17
        default:
            idx = (i + 1) * 18 + 17;
            break;
    }

    pressure[idx] = halo_in[i];
}
"#.to_string()
}

/// Generate the read_interior kernel.
#[cfg(feature = "cuda-codegen")]
fn generate_read_interior() -> String {
    r#"// Read interior pressure to linear buffer for visualization
__global__ void read_interior(
    const float* __restrict__ pressure,
    float* __restrict__ output
) {
    int lx = threadIdx.x;
    int ly = threadIdx.y;

    if (lx >= 16 || ly >= 16) return;

    int src_idx = (ly + 1) * 18 + (lx + 1);
    int dst_idx = ly * 16 + lx;

    output[dst_idx] = pressure[src_idx];
}
"#.to_string()
}

/// Generate the apply_boundary_reflection kernel.
#[cfg(feature = "cuda-codegen")]
fn generate_apply_boundary_reflection() -> String {
    r#"// Apply boundary reflection for tiles at grid edges
// edge: 0=North, 1=South, 2=West, 3=East
__global__ void apply_boundary_reflection(
    float* __restrict__ pressure,
    int edge,
    float reflection_coeff
) {
    int i = threadIdx.x;
    if (i >= 16) return;

    int src_idx, dst_idx;
    switch (edge) {
        case 0:  // North - reflect row 1 to row 0
            src_idx = 1 * 18 + (i + 1);
            dst_idx = 0 * 18 + (i + 1);
            break;
        case 1:  // South - reflect row 16 to row 17
            src_idx = 16 * 18 + (i + 1);
            dst_idx = 17 * 18 + (i + 1);
            break;
        case 2:  // West - reflect col 1 to col 0
            src_idx = (i + 1) * 18 + 1;
            dst_idx = (i + 1) * 18 + 0;
            break;
        case 3:  // East - reflect col 16 to col 17
        default:
            src_idx = (i + 1) * 18 + 16;
            dst_idx = (i + 1) * 18 + 17;
            break;
    }

    pressure[dst_idx] = pressure[src_idx] * reflection_coeff;
}
"#.to_string()
}

// ============================================================================
// Packed Tile Kernels (fdtd_packed.cu equivalent)
// ============================================================================

/// Generate the complete CUDA source for packed tile kernels.
///
/// This generates CUDA code equivalent to `shaders/fdtd_packed.cu`:
/// - `exchange_all_halos`: Copy halos between adjacent tiles
/// - `fdtd_all_tiles`: Batched FDTD for all tiles in parallel
/// - `upload_tile_data`: Upload initial state to a tile
/// - `read_all_interiors`: Read all tile interiors to output
/// - `inject_impulse`: Add impulse to specific cell
/// - `apply_boundary_conditions`: Apply boundary conditions to edge tiles
#[cfg(feature = "cuda-codegen")]
pub fn generate_packed_kernels() -> String {
    let mut output = String::new();

    output.push_str(PACKED_KERNELS_HEADER);
    output.push_str("\nextern \"C\" {\n\n");

    output.push_str(&generate_exchange_all_halos());
    output.push('\n');

    output.push_str(&generate_fdtd_all_tiles());
    output.push('\n');

    output.push_str(&generate_upload_tile_data());
    output.push('\n');

    output.push_str(&generate_read_all_interiors());
    output.push('\n');

    output.push_str(&generate_inject_impulse());
    output.push('\n');

    output.push_str(&generate_apply_boundary_conditions());

    output.push_str("\n}  // extern \"C\"\n");

    output
}

/// Header comment for generated packed kernels.
pub const PACKED_KERNELS_HEADER: &str = r#"// CUDA Kernels for Packed Tile-Based FDTD Wave Simulation
// Generated by ringkernel-cuda-codegen from Rust DSL
//
// All tiles packed contiguously: [Tile(0,0)][Tile(1,0)]...[Tile(n,m)]
// Each tile is 18x18 floats (16x16 interior + 1-cell halo)
//
// Benefits:
// - Zero host<->GPU transfers during simulation
// - All tiles computed in parallel
// - Halo exchange is just GPU memory copies
"#;

/// Generate exchange_all_halos kernel.
#[cfg(feature = "cuda-codegen")]
fn generate_exchange_all_halos() -> String {
    r#"// Halo Exchange Kernel - copies all halo edges between adjacent tiles
__global__ void exchange_all_halos(
    float* __restrict__ packed_buffer,
    const unsigned int* __restrict__ copies,
    int num_copies
) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= num_copies) return;

    unsigned int src_idx = copies[idx * 2];
    unsigned int dst_idx = copies[idx * 2 + 1];

    packed_buffer[dst_idx] = packed_buffer[src_idx];
}
"#.to_string()
}

/// Generate fdtd_all_tiles kernel.
#[cfg(feature = "cuda-codegen")]
fn generate_fdtd_all_tiles() -> String {
    r#"// Batched FDTD Kernel - computes FDTD for ALL tiles in single launch
__global__ void fdtd_all_tiles(
    const float* __restrict__ packed_curr,
    float* __restrict__ packed_prev,
    int tiles_x,
    int tiles_y,
    int tile_size,
    int buffer_width,
    float c2,
    float damping
) {
    int tile_x = blockIdx.x;
    int tile_y = blockIdx.y;

    int lx = threadIdx.x;
    int ly = threadIdx.y;

    if (tile_x >= tiles_x || tile_y >= tiles_y) return;
    if (lx >= tile_size || ly >= tile_size) return;

    int tile_buffer_size = buffer_width * buffer_width;
    int tile_idx = tile_y * tiles_x + tile_x;
    int tile_offset = tile_idx * tile_buffer_size;

    int idx = tile_offset + (ly + 1) * buffer_width + (lx + 1);

    float p = packed_curr[idx];
    float p_prev = packed_prev[idx];

    float p_n = packed_curr[idx - buffer_width];
    float p_s = packed_curr[idx + buffer_width];
    float p_w = packed_curr[idx - 1];
    float p_e = packed_curr[idx + 1];

    float laplacian = p_n + p_s + p_e + p_w - 4.0f * p;
    float p_new = 2.0f * p - p_prev + c2 * laplacian;

    packed_prev[idx] = p_new * damping;
}
"#.to_string()
}

/// Generate upload_tile_data kernel.
#[cfg(feature = "cuda-codegen")]
fn generate_upload_tile_data() -> String {
    r#"// Upload Initial State - copies initial data to packed buffer
__global__ void upload_tile_data(
    float* __restrict__ packed_buffer,
    const float* __restrict__ staging,
    int tile_x,
    int tile_y,
    int tiles_x,
    int buffer_width
) {
    int lx = threadIdx.x;
    int ly = threadIdx.y;

    if (lx >= buffer_width || ly >= buffer_width) return;

    int tile_buffer_size = buffer_width * buffer_width;
    int tile_idx = tile_y * tiles_x + tile_x;
    int tile_offset = tile_idx * tile_buffer_size;

    int local_idx = ly * buffer_width + lx;
    int global_idx = tile_offset + local_idx;

    packed_buffer[global_idx] = staging[local_idx];
}
"#.to_string()
}

/// Generate read_all_interiors kernel.
#[cfg(feature = "cuda-codegen")]
fn generate_read_all_interiors() -> String {
    r#"// Read All Interiors - extracts all tile interiors for visualization
__global__ void read_all_interiors(
    const float* __restrict__ packed_buffer,
    float* __restrict__ output,
    int tiles_x,
    int tiles_y,
    int tile_size,
    int buffer_width,
    int grid_width,
    int grid_height
) {
    int gx = blockIdx.x * blockDim.x + threadIdx.x;
    int gy = blockIdx.y * blockDim.y + threadIdx.y;

    if (gx >= grid_width || gy >= grid_height) return;

    int tile_x = gx / tile_size;
    int tile_y = gy / tile_size;

    int lx = gx % tile_size;
    int ly = gy % tile_size;

    int tile_buffer_size = buffer_width * buffer_width;
    int tile_idx = tile_y * tiles_x + tile_x;
    int tile_offset = tile_idx * tile_buffer_size;
    int src_idx = tile_offset + (ly + 1) * buffer_width + (lx + 1);

    int dst_idx = gy * grid_width + gx;

    output[dst_idx] = packed_buffer[src_idx];
}
"#.to_string()
}

/// Generate inject_impulse kernel.
#[cfg(feature = "cuda-codegen")]
fn generate_inject_impulse() -> String {
    r#"// Inject Impulse - adds energy to specific cell
__global__ void inject_impulse(
    float* __restrict__ packed_buffer,
    int tile_x,
    int tile_y,
    int local_x,
    int local_y,
    int tiles_x,
    int buffer_width,
    float amplitude
) {
    int tile_buffer_size = buffer_width * buffer_width;
    int tile_idx = tile_y * tiles_x + tile_x;
    int tile_offset = tile_idx * tile_buffer_size;
    int idx = tile_offset + (local_y + 1) * buffer_width + (local_x + 1);

    packed_buffer[idx] += amplitude;
}
"#.to_string()
}

/// Generate apply_boundary_conditions kernel.
#[cfg(feature = "cuda-codegen")]
fn generate_apply_boundary_conditions() -> String {
    r#"// Apply Boundary Conditions - handles domain edges
__global__ void apply_boundary_conditions(
    float* __restrict__ packed_buffer,
    int tiles_x,
    int tiles_y,
    int tile_size,
    int buffer_width,
    float reflection_coeff
) {
    int edge = blockIdx.x;
    int idx = threadIdx.x;

    int tile_buffer_size = buffer_width * buffer_width;

    if (edge == 0) {
        // North boundary: tiles with tile_y == 0
        int tile_x = idx / tile_size;
        int cell_x = idx % tile_size;
        if (tile_x >= tiles_x) return;

        int tile_idx = 0 * tiles_x + tile_x;
        int tile_offset = tile_idx * tile_buffer_size;
        int src_idx = tile_offset + 1 * buffer_width + (cell_x + 1);
        int dst_idx = tile_offset + 0 * buffer_width + (cell_x + 1);
        packed_buffer[dst_idx] = packed_buffer[src_idx] * reflection_coeff;
    }
    else if (edge == 1) {
        // South boundary: tiles with tile_y == tiles_y - 1
        int tile_x = idx / tile_size;
        int cell_x = idx % tile_size;
        if (tile_x >= tiles_x) return;

        int tile_idx = (tiles_y - 1) * tiles_x + tile_x;
        int tile_offset = tile_idx * tile_buffer_size;
        int src_idx = tile_offset + tile_size * buffer_width + (cell_x + 1);
        int dst_idx = tile_offset + (tile_size + 1) * buffer_width + (cell_x + 1);
        packed_buffer[dst_idx] = packed_buffer[src_idx] * reflection_coeff;
    }
    else if (edge == 2) {
        // West boundary: tiles with tile_x == 0
        int tile_y = idx / tile_size;
        int cell_y = idx % tile_size;
        if (tile_y >= tiles_y) return;

        int tile_idx = tile_y * tiles_x + 0;
        int tile_offset = tile_idx * tile_buffer_size;
        int src_idx = tile_offset + (cell_y + 1) * buffer_width + 1;
        int dst_idx = tile_offset + (cell_y + 1) * buffer_width + 0;
        packed_buffer[dst_idx] = packed_buffer[src_idx] * reflection_coeff;
    }
    else if (edge == 3) {
        // East boundary: tiles with tile_x == tiles_x - 1
        int tile_y = idx / tile_size;
        int cell_y = idx % tile_size;
        if (tile_y >= tiles_y) return;

        int tile_idx = tile_y * tiles_x + (tiles_x - 1);
        int tile_offset = tile_idx * tile_buffer_size;
        int src_idx = tile_offset + (cell_y + 1) * buffer_width + tile_size;
        int dst_idx = tile_offset + (cell_y + 1) * buffer_width + (tile_size + 1);
        packed_buffer[dst_idx] = packed_buffer[src_idx] * reflection_coeff;
    }
}
"#.to_string()
}

// ============================================================================
// Fallback implementations (when cuda-codegen is not enabled)
// ============================================================================

#[cfg(not(feature = "cuda-codegen"))]
pub fn generate_tile_kernels() -> String {
    "// CUDA codegen not enabled - use handwritten shaders/fdtd_tile.cu".to_string()
}

#[cfg(not(feature = "cuda-codegen"))]
pub fn generate_packed_kernels() -> String {
    "// CUDA codegen not enabled - use handwritten shaders/fdtd_packed.cu".to_string()
}

// ============================================================================
// Tests
// ============================================================================

#[cfg(test)]
mod tests {
    #[allow(unused_imports)]
    use super::*;

    #[test]
    #[cfg(feature = "cuda-codegen")]
    fn test_tile_kernels_structure() {
        let source = generate_tile_kernels();

        // Check all kernels are present
        assert!(source.contains("fdtd_tile_step"), "Missing fdtd_tile_step");
        assert!(source.contains("extract_halo"), "Missing extract_halo");
        assert!(source.contains("inject_halo"), "Missing inject_halo");
        assert!(source.contains("read_interior"), "Missing read_interior");
        assert!(source.contains("apply_boundary_reflection"), "Missing apply_boundary_reflection");

        // Check extern "C" wrapper
        assert!(source.contains("extern \"C\""), "Missing extern C");

        // Check FDTD kernel has correct structure
        assert!(source.contains("__global__ void fdtd_tile_step"));
        assert!(source.contains("threadIdx.x"));
        assert!(source.contains("threadIdx.y"));
        assert!(source.contains("buffer_width = 18") || source.contains("* 18"));
    }

    #[test]
    #[cfg(feature = "cuda-codegen")]
    fn test_packed_kernels_structure() {
        let source = generate_packed_kernels();

        // Check all kernels are present
        assert!(source.contains("exchange_all_halos"), "Missing exchange_all_halos");
        assert!(source.contains("fdtd_all_tiles"), "Missing fdtd_all_tiles");
        assert!(source.contains("upload_tile_data"), "Missing upload_tile_data");
        assert!(source.contains("read_all_interiors"), "Missing read_all_interiors");
        assert!(source.contains("inject_impulse"), "Missing inject_impulse");
        assert!(source.contains("apply_boundary_conditions"), "Missing apply_boundary_conditions");

        // Check batched FDTD uses blockIdx
        assert!(source.contains("blockIdx.x"), "Missing blockIdx usage");
        assert!(source.contains("blockIdx.y"), "Missing blockIdx.y usage");
    }

    #[test]
    #[cfg(feature = "cuda-codegen")]
    fn test_fdtd_tile_step_matches_handwritten() {
        let generated = generate_fdtd_tile_step();

        // Verify key structural elements match handwritten version
        assert!(generated.contains("const float* __restrict__ pressure"));
        assert!(generated.contains("float* __restrict__ pressure_prev"));
        assert!(generated.contains("float c2"));
        assert!(generated.contains("float damping"));
        assert!(generated.contains("if (lx >= 16 || ly >= 16) return;"));
        assert!(generated.contains("idx = (ly + 1) * buffer_width + (lx + 1)")
            || generated.contains("(ly + 1) * 18 + (lx + 1)"));
        assert!(generated.contains("laplacian"));
        assert!(generated.contains("* damping"));

        println!("Generated fdtd_tile_step:\n{}", generated);
    }

    #[test]
    #[cfg(feature = "cuda-codegen")]
    fn test_generated_vs_handwritten_tile() {
        let generated = generate_tile_kernels();
        let handwritten = include_str!("../shaders/fdtd_tile.cu");

        // Count kernels in both
        let gen_kernel_count = generated.matches("__global__").count();
        let hw_kernel_count = handwritten.matches("__global__").count();

        assert_eq!(gen_kernel_count, hw_kernel_count,
            "Kernel count mismatch: generated={}, handwritten={}",
            gen_kernel_count, hw_kernel_count);
    }

    #[test]
    #[cfg(feature = "cuda-codegen")]
    fn test_generated_vs_handwritten_packed() {
        let generated = generate_packed_kernels();
        let handwritten = include_str!("../shaders/fdtd_packed.cu");

        // Count kernels in both
        let gen_kernel_count = generated.matches("__global__").count();
        let hw_kernel_count = handwritten.matches("__global__").count();

        assert_eq!(gen_kernel_count, hw_kernel_count,
            "Kernel count mismatch: generated={}, handwritten={}",
            gen_kernel_count, hw_kernel_count);
    }
}
